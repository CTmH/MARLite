agent_group_config:
  type: "ObsGNNComm"
  agent_list:
    red_0: model_0
    red_1: model_0
    red_2: model_0
    red_3: model_0
    red_4: model_0
    red_5: model_0
    red_6: model_0
    red_7: model_0
    red_8: model_0
    red_9: model_0
    red_10: model_0
    red_11: model_0
    red_12: model_0
    red_13: model_0
    red_14: model_0
    red_15: model_0
    red_16: model_0
    red_17: model_0
    red_18: model_0
    red_19: model_0
    red_20: model_0
    red_21: model_0
    red_22: model_0
    red_23: model_0
    red_24: model_0
    red_25: model_0
    red_26: model_0
    red_27: model_0
    red_28: model_0
    red_29: model_0
    red_30: model_0
    red_31: model_0
    red_32: model_0
    red_33: model_0
    red_34: model_0
    red_35: model_0
  model_configs:
    model_0:
      feature_extractor:
        model_type: "Custom"
        layers:
        - type: Conv2d
          in_channels: 37
          out_channels: 16
          kernel_size: 1
          stride: 1
          padding: 0
        - type: BatchNorm2d
          num_features: 16
        - type: GELU
        - type: Conv2d
          in_channels: 16
          out_channels: 4
          kernel_size: 1
          stride: 1
          padding: 0
        - type: BatchNorm2d
          num_features: 4
        - type: GELU
        - type: Flatten
        - type: Linear
          in_features: 676
          out_features: 64
      encoder:
        model_type: "SimpleResAttSeqEnc"
        input_dim: 64
        embed_dim: 128
        output_dim: 128
        num_heads: 4
        max_seq_len: 8
        dropout: 0.0
      decoder:
        model_type: "Custom"
        layers:
        - type: GELU
        - type: Linear
          in_features: 256
          out_features: 21
  graph_builder_config:
    type: "PartialMAgent"
    binary_agent_id_dim: [5, 6, 7, 8, 9, 10, 11, 12, 13, 14]
    agent_presence_dim: [1, 3] # Predator: 1, Prey: 3
    comm_distance: 8
    distance_metric: "cityblock"
    n_workers: 32
    n_subgraphs: 8
    valid_node_list: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]
    target_node_list: [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71]
    update_interval: 5
  graph_model_config:
    model_type: "GAT"
    input_dim: 64
    hidden_dim: 128
    output_dim: 128
    head_conv1: 4
    head_conv2: 4
    dropout: 0.5
    activation: ELU
  optimizer:
    type: "Adam"
    lr: 0.001
    weight_decay: 0.00001
  lr_scheduler:
    type: "ReduceLROnPlateau"
    mode: 'max'
    factor: 0.5
    patience: 4
    min_lr: 0.00001

env_config:
  module_name: "magent2.environments"
  env_name: "battle_v4"
  env_config:
    map_size: 32
    step_reward: -0.00
    dead_penalty: -1.0
    attack_penalty: -0.05
    attack_opponent_reward: 1.0
    extra_features: true
  wrapper_config:
    type: battle
    opp_obs_queue_len: 1
    channel_first: true
    vector_state: false
    opponent_agent_group_config:
      type: "MAgentBattle"
      agent_list:
        blue_0: policy
        blue_1: policy
        blue_2: policy
        blue_3: policy
        blue_4: policy
        blue_5: policy
        blue_6: policy
        blue_7: policy
        blue_8: policy
        blue_9: policy
        blue_10: policy
        blue_11: policy
        blue_12: policy
        blue_13: policy
        blue_14: policy
        blue_15: policy
        blue_16: policy
        blue_17: policy
        blue_18: policy
        blue_19: policy
        blue_20: policy
        blue_21: policy
        blue_22: policy
        blue_23: policy
        blue_24: policy
        blue_25: policy
        blue_26: policy
        blue_27: policy
        blue_28: policy
        blue_29: policy
        blue_30: policy
        blue_31: policy
        blue_32: policy
        blue_33: policy
        blue_34: policy
        blue_35: policy

epsilon_scheduler:
  type: "linear"
  start_value: 1.0
  end_value: 0.05
  decay_steps: 100

sample_ratio_scheduler:
  type: "logarithmic"
  start_value: 0.66
  end_value: 0.33
  decay_steps: 100

critic_config:
  type: "QMixer"
  model:
    model_type: QMixModel
    state_shape: 256
    input_dim: 36
    qmix_hidden_dim: 256
    hypernet_layers: 2
    hyper_hidden_dim: 256
  feature_extractor:
    model_type: "Custom"
    layers:
      - type: Conv2d
        in_channels: 37
        out_channels: 32
        kernel_size: 9
        stride: 4
        padding: 4
      - type: BatchNorm2d
        num_features: 32
      - type: GELU
      - type: Conv2d
        in_channels: 32
        out_channels: 64
        kernel_size: 4
        stride: 2
        padding: 1
      - type: GELU
      - type: BatchNorm2d
        num_features: 64
      - type: AdaptiveAvgPool2d
        output_size: [2,2]
      - type: Flatten
  optimizer:
    type: "Adam"
    lr: 0.001
    weight_decay: 0.00001
  lr_scheduler:
    type: "ReduceLROnPlateau"
    mode: 'max'
    factor: 0.5
    patience: 4
    min_lr: 0.00001

rollout_config:
  manager_type: "multi-process"
  worker_type: "multi-process"
  n_workers: 4
  n_episodes: 16
  n_eval_episodes: 16
  traj_len: 8
  episode_limit: 200
  victory_checker: "default"
  device: "cpu"

replaybuffer_config:
  type: "Prioritized"
  capacity: 3200
  traj_len: 8
  priority_attr: "all_agents_sum_rewards"

analyzer_config:
  type: "default"

trainer_config:
  type: "GraphQMIX"
  gamma: 0.95
  eval_epsilon: 0.005
  eval_episodes_to_replay_ratio: 0.25
  workdir: "./test/results/partial_gat_bf/"
  train_device: "cpu"
  use_data_parallel: false
  compile_models: false
  n_workers: 0

  train_args:
    epochs: 100
    target_first_metric: 10000000
    eval_interval: 4
    update_target_interval: 2
    batch_size: 256
    learning_times_per_epoch: 1