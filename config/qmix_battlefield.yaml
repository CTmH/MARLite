agent_group_config:
  type: "QMIX"
  agent_list:
    red_0: model1
    red_1: model1
    red_2: model1
    red_3: model1
    red_4: model1
    red_5: model1
    red_6: model1
    red_7: model1
    red_8: model1
    red_9: model1
    red_10: model1
    red_11: model1
  model_configs:
    model1:
      feature_extractor:
        model_type: "Custom"
        layers:
        - type: Permute
          dims: [0, 3, 1, 2]
        - type: Conv2d
          in_channels: 5
          out_channels: 16
          kernel_size: 3
          stride: 1
          padding: 1
        - type: ReLU
        - type: MaxPool2d
          kernel_size: 2
          stride: 2
        - type: Conv2d
          in_channels: 16
          out_channels: 32
          kernel_size: 3
          stride: 1
          padding: 1
        - type: ReLU
        - type: MaxPool2d
          kernel_size: 2
          stride: 2
        - type: Conv2d
          in_channels: 32
          out_channels: 64
          kernel_size: 3
          stride: 1
          padding: 1
        - type: ReLU
        - type: AdaptiveAvgPool2d
          output_size: [1, 1]
        - type: Flatten
        - type: Linear
          in_features: 64
          out_features: 64
      model:
        model_type: "RNN"
        input_shape: 64
        rnn_hidden_dim: 128
        rnn_layers: 1
        output_shape: 13
  optimizer:
    type: "Adam"
    lr: 0.0005
    weight_decay: 0.0001

env_config:
  module_name: "custom"
  env_name: "battlefield"
  opponent_agent_group_config:
    type: "Random"
    agent_list:
      blue_0: random1
      blue_1: random1
      blue_2: random1
      blue_3: random1
      blue_4: random1
      blue_5: random1
      blue_6: random1
      blue_7: random1
      blue_8: random1
      blue_9: random1
      blue_10: random1
      blue_11: random1
  opp_obs_queue_len: 5

epsilon_scheduler:
  type: "logarithmic"
  start_value: 1.0
  end_value: 0.05
  decay_steps: 100

sample_ratio_scheduler:
  type: "linear"
  start_value: 1.0
  end_value: 0.3
  decay_steps: 100

critic_config:
  type: "QMIX"
  state_shape: 256
  input_dim: 156
  qmix_hidden_dim: 1024
  hyper_hidden_dim: 256
  feature_extractor:
    model_type: "Custom"
    layers:
      - type: Permute
        dims: [0, 3, 1, 2]
      # Conv 1
      - type: Conv2d
        in_channels: 5
        out_channels: 32
        kernel_size: 3
        stride: 1
        padding: 1
      - type: ReLU
      - type: MaxPool2d
        kernel_size: 2
        stride: 2
        padding: 0
      # Conv 2
      - type: Conv2d
        in_channels: 32
        out_channels: 64
        kernel_size: 3
        stride: 1
        padding: 1
      - type: ReLU
      - type: MaxPool2d
        kernel_size: 2
        stride: 2
        padding: 0
      # Conv 3
      - type: Conv2d
        in_channels: 64
        out_channels: 128
        kernel_size: 3
        stride: 1
        padding: 1
      - type: ReLU
      - type: MaxPool2d
        kernel_size: 2
        stride: 2
        padding: 0
      # Conv 4
      - type: Conv2d
        in_channels: 128
        out_channels: 256
        kernel_size: 3
        stride: 1
        padding: 1
      - type: ReLU
      - type: MaxPool2d
        kernel_size: 2
        stride: 2
        padding: 0
      # Linear
      - type: Flatten
      - type: Linear
        in_features: 6400  # 经过三次池化后，特征图大小为 10x10
        out_features: 512
      - type: ReLU
      - type: Dropout
        p: 0.5
      - type: Linear
        in_features: 512
        out_features: 256
  optimizer:
    type: "Adam"
    lr: 0.0005
    weight_decay: 0.0001

rollout_config:
  manager_type: "multi-thread"
  worker_type: "multi-thread"
  n_workers: 1
  n_episodes: 1000
  n_eval_episodes: 100
  traj_len: 5
  episode_limit: 500
  device: "cpu"

replaybuffer_config:
  type: "Prioritized"
  capacity: 50000
  traj_len: 5
  priority_attr: all_agents_sum_rewards
  alpha: 0.7

trainer_config:
  type: "QMIX"
  gamma: 0.95
  eval_epsilon: 0.01
  workdir: "./results/qmix_battlefield"
  train_device: "cpu"
  eval_device: "cpu"
  
  train_args:
    epochs: 100
    target_reward: 100
    eval_interval: 1
    batch_size: 128
    learning_times_per_epoch: 1